{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXhjY9dGwJls"
      },
      "source": [
        "NOMBRES: Diego Alberto\n",
        "\n",
        "APELLIDOS: Leiva Pérez\n",
        "\n",
        "CARNE: 21752\n",
        "\n",
        "FECHA: 25/09/2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7nnvCPLh5Ms"
      },
      "source": [
        "**Ejercicio 1**\n",
        "Cree un dataset con noticias (Pueden ser falsas) para poder clasificarlas por categoria (Recomiendo que sea binaria para su comodidad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED=42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generar_dataset_noticias(news_sample=120, seed=42, as_df=True, csv_path=None):\n",
        "    \"\"\"\n",
        "    Generate a synthetic dataset of news headlines in Spanish.\n",
        "    Each headline is labeled as real (1) or fake (0).\n",
        "    Params:\n",
        "        news_sample (int): Number of samples per class (real and fake).\n",
        "        seed (int): Random seed for reproducibility.\n",
        "        as_df (bool): If True, return a pandas DataFrame; else return a list of dicts.\n",
        "        csv_path (str or None): If provided, save the DataFrame to this CSV path\n",
        "                                (only if as_df is True).\n",
        "    Returns:\n",
        "        pd.DataFrame or list of dicts: The generated dataset.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "    entidades = [\n",
        "        \"Guatemala\",\"Quetzaltenango\",\"Antigua Guatemala\",\"Universidad del Valle\",\n",
        "        \"Congreso\",\"Ministerio de Salud\",\"Municipalidad\",\"CONRED\",\n",
        "        \"Liga Nacional\",\"SAT\",\"Banco de Guatemala\",\"CIV\"\n",
        "    ]\n",
        "    temas = [\n",
        "        \"salud\",\"educación\",\"seguridad\",\"economía\",\"transporte\",\n",
        "        \"tecnología\",\"deporte\",\"ambiente\",\"cultura\",\"energía\"\n",
        "    ]\n",
        "\n",
        "    plantillas_reales = [\n",
        "        \"El {ent} anuncia nuevas medidas para {tema}\",\n",
        "        \"{ent} presenta informe trimestral sobre {tema}\",\n",
        "        \"{ent} inaugura proyecto de {tema} en {lugar}\",\n",
        "        \"{ent} confirma operativo de control en {lugar}\",\n",
        "        \"{ent} reporta incremento del {porc}% en {tema}\",\n",
        "        \"{ent} establece convenio para fortalecer {tema}\",\n",
        "        \"{ent} publica calendario oficial de {tema}\",\n",
        "        \"{ent} implementa campaña nacional de {tema}\",\n",
        "        \"{ent} actualiza protocolo de {tema}\",\n",
        "        \"{ent} realiza jornada de {tema} en {lugar}\",\n",
        "    ]\n",
        "    plantillas_falsas = [\n",
        "        \"Confirman llegada de extraterrestres a {lugar}\",\n",
        "        \"Descubren fórmula para vivir {anios} años\",\n",
        "        \"Anuncian que los dragones regresarán en {lugar}\",\n",
        "        \"Habilitan viajes en el tiempo desde {lugar}\",\n",
        "        \"Revelan que la Tierra es hueca bajo {lugar}\",\n",
        "        \"Presentan dispositivo que lee la mente en {lugar}\",\n",
        "        \"Aseguran que el café cura todas las enfermedades\",\n",
        "        \"Afirman que un meteorito de oro cayó en {lugar}\",\n",
        "        \"Garantizan ganar la lotería con un método secreto\",\n",
        "        \"Prometen internet gratuito mundial desde mañana\",\n",
        "    ]\n",
        "\n",
        "    def sreal():\n",
        "        \"\"\"\n",
        "        Generate a synthetic real news headline.\n",
        "        \"\"\"\n",
        "        ent = random.choice(entidades)\n",
        "        t   = random.choice(temas)\n",
        "        tpl = random.choice(plantillas_reales)\n",
        "        txt = tpl.format(\n",
        "            ent=ent, tema=t, lugar=random.choice(entidades),\n",
        "            porc=random.randint(3,35)\n",
        "        )\n",
        "        return txt + \".\"\n",
        "\n",
        "    def sfake():\n",
        "        \"\"\"\n",
        "        Generate a synthetic fake news headline.\n",
        "        \"\"\"\n",
        "        tpl = random.choice(plantillas_falsas)\n",
        "        txt = tpl.format(\n",
        "            lugar=random.choice(entidades),\n",
        "            anios=random.randint(150,500)\n",
        "        )\n",
        "        return txt + \".\"\n",
        "\n",
        "    data = []\n",
        "    for _ in range(news_sample):\n",
        "        data.append({\"text\": sreal(), \"label\": 1})\n",
        "    for _ in range(news_sample):\n",
        "        data.append({\"text\": sfake(), \"label\": 0})\n",
        "\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # Generate dataframe or list of dicts\n",
        "    if as_df:\n",
        "        df = pd.DataFrame(data)\n",
        "        if csv_path:\n",
        "            df.to_csv(csv_path, index=False)\n",
        "            print(f\"CSV guardado en: {csv_path}\")\n",
        "        return df\n",
        "    else:\n",
        "        # Lista de dicts\n",
        "        for r in data[:5]:\n",
        "            print(r)\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf3GX1_liFXd"
      },
      "outputs": [],
      "source": [
        "df = generar_dataset_noticias(news_sample=120, as_df=True, csv_path=\"dataset_es.csv\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "df['label'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title(\"Distribución de etiquetas\")\n",
        "plt.xlabel(\"Etiquetas\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzRf6sNDiF1v"
      },
      "source": [
        "**Ejercicio 2**\n",
        " En base al dataset creado anteriormente Realizar una CNN para poder clasificarla de manera automatica, debe de realizarla con 3 filtros y dejar cada una de ellas como una implementacion nuevo por lo que recomiendo trabajarlo por funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assert dataset labels\n",
        "assert set([\"text\",\"label\"]).issubset(df.columns)\n",
        "df[\"label\"] = df[\"label\"].astype(int)\n",
        "\n",
        "# Stratified split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    df[\"text\"].values, \n",
        "    df[\"label\"].values, \n",
        "    test_size=0.1,\n",
        "    random_state=SEED, \n",
        "    stratify=df[\"label\"].values\n",
        "    )\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp,\n",
        "    y_temp,\n",
        "    test_size=0.1111,\n",
        "    random_state=SEED,\n",
        "    stratify=y_temp\n",
        "    )\n",
        "\n",
        "# Tensors with tensorflow datasets\n",
        "train_raw = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_raw   = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "test_raw  = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "MAX_TOKENS = 20000\n",
        "SEQ_LEN = 32\n",
        "EMB_DIM = 128\n",
        "DROPOUT = 0.5\n",
        "EPOCHS = 8\n",
        "LR = 1e-3\n",
        "\n",
        "# Vectorization and preprocessing\n",
        "vectorizer = layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LEN,\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        ")\n",
        "vectorizer.adapt(train_raw.map(lambda x,y:x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(x,y):\n",
        "    \"\"\"\n",
        "    Preprocess function to vectorize text and cast labels.\n",
        "    Params:\n",
        "        x (tf.Tensor): Input text tensor.\n",
        "        y (tf.Tensor): Input label tensor.\n",
        "    Returns:\n",
        "        tuple: (vectorized text tensor, casted label tensor)\n",
        "    \"\"\"\n",
        "    return vectorizer(x), tf.cast(y, tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data pipeline with shuffling, batching, and prefetching\n",
        "train = (train_raw\n",
        "         .map(preprocess)\n",
        "         .cache()\n",
        "         .shuffle(5000, seed=SEED)\n",
        "         .batch(BATCH_SIZE, drop_remainder=True)\n",
        "         .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "val   = (val_raw\n",
        "         .map(preprocess)\n",
        "         .cache()\n",
        "         .batch(BATCH_SIZE)\n",
        "         .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "test  = (test_raw\n",
        "         .map(preprocess)\n",
        "         .cache()\n",
        "         .batch(BATCH_SIZE)\n",
        "         .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# Y true for evaluation\n",
        "y_true = y_test.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_cnn(kernel_size:int)->tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Build and compile a CNN model with the specified kernel size.\n",
        "    Params:\n",
        "        kernel_size (int): Size of the convolutional kernel.\n",
        "    Returns:\n",
        "        tf.keras.Model: The compiled CNN model.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(SEQ_LEN,), dtype=tf.int32)\n",
        "    x = layers.Embedding(input_dim=MAX_TOKENS, output_dim=EMB_DIM, name=\"emb\")(inputs)\n",
        "    x = layers.Conv1D(filters=128, kernel_size=kernel_size, activation=\"relu\", padding=\"valid\", name=f\"conv_k{kernel_size}\")(x)\n",
        "    x = layers.GlobalMaxPooling1D(name=f\"gmp_k{kernel_size}\")(x)\n",
        "    x = layers.Dropout(DROPOUT)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(DROPOUT)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs, outputs, name=f\"cnn_k{kernel_size}\")\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and train models with different kernel sizes\n",
        "kernel_sizes = [3,4,5]\n",
        "histories = {}\n",
        "\n",
        "# Loop over kernel sizes\n",
        "for k in kernel_sizes:\n",
        "    # Train model\n",
        "    model = build_cnn(k)\n",
        "    cb = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "    h = model.fit(train, validation_data=val, epochs=EPOCHS, callbacks=cb, verbose=0)\n",
        "    histories[k] = h\n",
        "\n",
        "    # Evaluate model\n",
        "    print(f\"--- Modelo con filtro de tamaño {k} ---\")\n",
        "    y_prob = model.predict(test, verbose=0).ravel()\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Curvas\n",
        "plt.figure()\n",
        "for k,h in histories.items():\n",
        "    plt.plot(h.history[\"val_accuracy\"], label=f\"filtro={k}\")\n",
        "plt.title(\"Accuracy de validación por tamaño de filtro\")\n",
        "plt.xlabel(\"Época\"); plt.ylabel(\"Accuracy\"); plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "for k,h in histories.items():\n",
        "    plt.plot(h.history[\"val_loss\"], label=f\"filtro={k}\")\n",
        "plt.title(\"Pérdida de validación por tamaño de filtro\")\n",
        "plt.xlabel(\"Época\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CUzcKDqF0wJ"
      },
      "source": [
        "**Ejercicio 3**\n",
        "Debe de discutir sus resultados y realizar una comparacion de porque varia dependiendo del filtro que se use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los tres modelos alcanzaron **1.00** de accuracy y F1 en test para `k=3,4,5`. Esto indica un **efecto techo** causado por la alta separabilidad del dataset sintético y la brevedad de los titulares: existen *“palabras gatillo”* distintivas por clase y cualquier ventana 3–5 captura los mismos n-gramas útiles. La única diferencia observable aparece en la **pérdida**: es ligeramente menor con `k=5`, lo que refleja **mayor confianza** en las predicciones ya correctas, no más aciertos. En otras palabras, cambia la **calibración** de las probabilidades, no la precisión.\n",
        "\n",
        "En escenarios más naturales sí esperaría variación por tamaño de filtro: `k=3` suele favorecer expresiones cortas y señales locales, `k=5` puede capturar dependencias algo más largas en oraciones compuestas, y `k=4` funciona como equilibrio."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp-lab6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
